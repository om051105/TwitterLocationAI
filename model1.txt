# -*- coding: utf-8 -*-
"""model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nAQ8MRiqKPPcssfRTCpGNVqbwsYsshM
"""



pip install keras-pos-embd

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os

import itertools

# %matplotlib inline
import matplotlib.pyplot as plt # for plottiing
import tensorflow as tf

from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.metrics import confusion_matrix

from tensorflow import keras
layers = keras.layers
models = keras.models
from keras_pos_embd import TrigPosEmbedding

# This code was tested with TensorFlow v1.8
print("You have TensorFlow version", tf.__version__)

"""get the data"""
############################################################ for data
from google.colab import drive # comment this for working on system
drive.mount('/content/drive') # comment this for working on system

#data = pd.read_csv (r'/content/drive/My Drive/Social_Media_DATA_MINING/train1.csv') # ram
# put your csv file here 
data = pd.read_csv (r'/content/drive/My Drive/text_classification/archive/train1.csv')  #mukul # comment this for working on system 

# data = pd.read_csv (r'/content/train1.csv')  #mukul
################################################################ for data
data.head()

data['city'].value_counts()

train_size = int(len(data) * .8)
print ("Train size: %d" % train_size)
print ("Test size: %d" % (len(data) - train_size))

def train_test_split(data, train_size):
    train = data[:train_size]
    test = data[train_size:]
    return train, test

train_cat, test_cat = train_test_split(data['city'], train_size)
train_country_cat, test_country_cat = train_test_split(data['country'], train_size)
train_text, test_text = train_test_split(data['text'], train_size)

max_words = 1000
tokenize = keras.preprocessing.text.Tokenizer(num_words=max_words, 
                                              char_level=False,oov_token='UNK')
char_tokenize = keras.preprocessing.text.Tokenizer(num_words=None, 
                                              char_level=True,oov_token='UNK')

tokenize.fit_on_texts(train_text) # fit tokenizer to our training text data
char_tokenize.fit_on_texts(train_text)
x_char_train = char_tokenize.texts_to_sequences(train_text)
x_char_test = char_tokenize.texts_to_sequences(test_text)
x_train = tokenize.texts_to_sequences(train_text)
x_test = tokenize.texts_to_sequences(test_text)
# print(x_train.shape)
# print(tokenize.word_index)
print(tokenize.document_count)
print(train_text)
print(x_train[0])
print(x_char_train[0])
maxlen = max([len(x) for x in x_train])
print(maxlen)

from tensorflow.keras.preprocessing.sequence import pad_sequences
# pad sequences 
max_sequence_len = 30
x_train = np.array(pad_sequences(x_train, maxlen=max_sequence_len, padding='post'))
# pad sequences 
max_sequence_len = 140
x_char_train = np.array(pad_sequences(x_char_train, maxlen=max_sequence_len, padding='post'))

max_sequence_len = 140
x_char_test = np.array(pad_sequences(x_char_test, maxlen=max_sequence_len, padding='post'))
max_sequence_len = 30
x_test = np.array(pad_sequences(x_test, maxlen=max_sequence_len, padding='post'))

class LabelEncoderExt(object):
    def __init__(self):
        """
        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]
        Unknown will be added in fit and transform will take care of new item. It gives unknown class id
        """
        self.label_encoder = LabelEncoder()
        # self.classes_ = self.label_encoder.classes_

    def fit(self, data_list):
        """
        This will fit the encoder for all the unique values and introduce unknown value
        :param data_list: A list of string
        :return: self
        """
        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])
        self.classes_ = self.label_encoder.classes_

        return self

    def transform(self, data_list):
        """
        This will transform the data_list to id list where the new values get assigned to Unknown class
        :param data_list:
        :return:
        """
        new_data_list = list(data_list)
        for unique_item in np.unique(data_list):
            if unique_item not in self.label_encoder.classes_:
                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]

        return self.label_encoder.transform(new_data_list)

# Use sklearn utility to convert label strings to numbered index
encoder = LabelEncoderExt()
encoder.fit(train_cat)
y_train = encoder.transform(train_cat)
y_test = encoder.transform(test_cat)

encoder_country = LabelEncoderExt()
encoder_country.fit(train_country_cat)
y_country_train = encoder_country.transform(train_country_cat)
y_country_test = encoder_country.transform(test_country_cat)

# Converts the labels to a one-hot representation
num_classes = np.max(y_train) + 1
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

num_classes_country = np.max(y_country_train) + 1
y_country_train = keras.utils.to_categorical(y_country_train, num_classes_country)
y_country_test = keras.utils.to_categorical(y_country_test, num_classes_country)

# Inspect the dimenstions of our training and test data (this is helpful to debug)
y_train = np.array(y_train)
x_train = np.array(x_train)
# x_train = np.array(x_train)
# x_train = np.array(x_train)
print('x_train shape:', x_train.shape, x_train.dtype)
print('x_test shape:', x_test.shape)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)
print('y_train shape:', y_country_train.shape)
print('y_test shape:', y_country_test.shape)
print(num_classes,num_classes_country)
# x_test = tf.convert_to_tensor(x_countaintrain)

"""make the model

position encoding

creating model as given in our paper
"""

#  num_words = 1000

drop = 0.3
word_input = keras.Input(
    shape=(None,), name="title"
)  # Variable-length sequence of ints
char_input = keras.Input(shape=(None,), name="body")  # Variable-length sequence of ints
d = 200    #dimension of a word
# Embed each word in the title into a 64-dimensional vector
word_features = layers.Embedding(1000, d)(word_input)

word_features = TrigPosEmbedding(
    output_dim=d,                      # The dimension of embeddings.
    mode=TrigPosEmbedding.MODE_ADD,  # Use `expand` mode
)(word_features)
# word_features = layers.add([word_features,pos_encoding])
print(word_features.shape)

# CHARS getting character encoding here instead for now only using words
char_features = layers.Embedding(1000, 100)(char_input)
print(char_features.shape, " char features")

#CHARS convolution for characters filters=100 kernel=3
res_conv = tf.keras.layers.Conv1D(64, 3, activation='relu',input_shape=(None,128))(char_features)
print(res_conv.shape, " res conv shape")

#CHARS max pooling layer after conv1D
res_pool = tf.keras.layers.MaxPooling1D(pool_size=3,strides=1, padding='valid')(res_conv)
print(res_pool.shape, " res pool shape")

#############################################
# res_conv = tf.keras.layers.Conv1D(64, 4, activation='relu',input_shape=(None,128))(res_pool)
# print(res_conv.shape, " res conv shape")

# #CHARS max pooling layer after conv1D
# res_pool = tf.keras.layers.MaxPooling1D(pool_size=3,strides=1, padding='valid')(res_conv)
# print(res_pool.shape, " res pool shape")

# res_conv = tf.keras.layers.Conv1D(64, 5, activation='relu',input_shape=(None,128))(res_pool)
# print(res_conv.shape, " res conv shape")

# #CHARS max pooling layer after conv1D
# res_pool = tf.keras.layers.MaxPooling1D(pool_size=3,strides=1, padding='valid')(res_conv)
# print(res_pool.shape, " res pool shape")

#############################################

#CHARS two multihead self-attention folloowed by feed forward neural network
multi = layers.MultiHeadAttention(num_heads=8,key_dim=2) 

char_features = multi(res_pool, res_pool)  # respool used two times for self-attention
print(word_features.shape)
p = 64
char_features1 = layers.Dense(4*p, activation='relu')(char_features) # feed forward network layer1
char_features1 = layers.Dense(p, activation='relu')(char_features1) # feed forward network layer2 
char_features = layers.add([char_features, char_features1])  # residual layer

print(char_features.shape, " char features shape")
multi = layers.MultiHeadAttention(num_heads=8,key_dim=2)
char_features = multi(char_features, char_features)  
char_features1 = layers.Dense(4*p, activation='relu')(char_features) # feed forward network layer1
char_features1 = layers.Dense(p, activation='relu')(char_features1) # feed forward network layer2 
char_features = layers.add([char_features, char_features1])  # residual layer

char_features = tf.reduce_sum(char_features, 1) #converts word matrix to a vector of dimension f 
print(char_features.shape, " char features shape")

#WORDS stack of two multihead self-attention with position-wise feed forward network
multi = layers.MultiHeadAttention(num_heads=10,key_dim=2) 

word_features = multi(word_features, word_features)  
print(word_features.shape)

word_features1 = layers.Dense(4*d, activation='relu')(word_features) # feed forward network layer1
word_features1 = layers.Dense(d, activation='relu')(word_features1) # feed forward network layer2 
word_features = layers.add([word_features, word_features1])  # residual layer

multi = layers.MultiHeadAttention(num_heads=10,key_dim=2) 

word_features = multi(word_features, word_features) 
print(word_features.shape)

word_features1 = layers.Dense(4*d, activation='relu')(word_features) # feed forward network layer1
word_features1 = layers.Dense(d, activation='relu')(word_features1) # feed forward network layer2 
word_features = layers.add([word_features, word_features1])  # residual layer
word_features = tf.reduce_sum(word_features, 1) #converts word matrix to a vector of dimension d  #equation 12

x = layers.concatenate([word_features, char_features]) # concatenate word vector and char vector
print(x.shape," x shape")
dropout = layers.Dropout(drop)(x)
city_pred = layers.Dense(num_classes, name="city",activation="softmax")(x)   #change value of 2 to number of possible cities
# city_pred = tf.keras.layers.Softmax(city_pred)
country_pred = layers.Dense(num_classes_country, name="country",activation="softmax")(x)  #change value of 2 to number of possible countries
# country_pred = tf.keras.layers.Softmax(country_pred)
model = keras.Model(
    inputs=[word_input,char_input],
    outputs=[city_pred, country_pred],
)
print(model.summary())

[print("i ",i.shape, i.dtype) for i in model.inputs]
[print("o ",o.shape, o.dtype) for o in model.outputs]
[print(l.name, l.input_shape, l.dtype) for l in model.layers]

keras.utils.plot_model(model, "multi_input_and_output_model.png", show_shapes=True)

"""implementing character side of model"""

model.compile(
    optimizer=keras.optimizers.Adam(0.0005),
    loss={
        "city": keras.losses.CategoricalCrossentropy(from_logits=True),
        "country": keras.losses.CategoricalCrossentropy(from_logits=True),
    },
    loss_weights=[1.0, 1.0],
    metrics = ['accuracy']
)

print(y_country_train)

history = model.fit(
    {"title": x_train,"body":x_char_train},
    {"city": y_train,"country":y_country_train},  # y_train in front of country should be changed 
    epochs=10,
    batch_size=512,
    verbose=1
)

# Evaluate the accuracy of our trained model
score = model.evaluate({"title": x_test,"body":x_char_test},
    {"city": y_test,"country": y_country_test}, batch_size=512, verbose=1)
print('Test loss:', score[0])
print('Test city accuracy:', score[3]*100)
print('Test country accuracy:', score[4]*100)

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  # plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])


print(history.history.keys())

"""model2"""

#  num_words = 1000

drop = 0.3
word_input = keras.Input(
    shape=(None,), name="title"
)  # Variable-length sequence of ints
char_input = keras.Input(shape=(None,), name="body")  # Variable-length sequence of ints
d = 200    #dimension of a word
# Embed each word in the title into a 64-dimensional vector
word_features = layers.Embedding(1000, d)(word_input)

word_features = TrigPosEmbedding(
    output_dim=d,                      # The dimension of embeddings.
    mode=TrigPosEmbedding.MODE_ADD,  # Use `expand` mode
)(word_features)
# word_features = layers.add([word_features,pos_encoding])
print(word_features.shape)

# CHARS getting character encoding here instead for now only using words
char_features = layers.Embedding(1000, 100)(char_input)
print(char_features.shape, " char features")

#CHARS convolution for characters filters=100 kernel=3
res_conv = tf.keras.layers.Conv1D(64, 3, activation='relu',input_shape=(None,128))(char_features)
print(res_conv.shape, " res conv shape")

#CHARS max pooling layer after conv1D
res_pool = tf.keras.layers.MaxPooling1D(pool_size=3,strides=1, padding='valid')(res_conv)
print(res_pool.shape, " res pool shape")

#CHARS two multihead self-attention folloowed by feed forward neural network
multi = layers.MultiHeadAttention(num_heads=1,key_dim=2) 

char_features = multi(res_pool, res_pool)  # respool used two times for self-attention
print(word_features.shape)
p = 64
char_features1 = layers.Dense(4*p, activation='relu')(char_features) # feed forward network layer1
char_features1 = layers.Dense(p, activation='relu')(char_features1) # feed forward network layer2 
char_features = layers.add([char_features, char_features1])  # residual layer

print(char_features.shape, " char features shape")
multi = layers.MultiHeadAttention(num_heads=1,key_dim=2)
char_features = multi(char_features, char_features)  
char_features1 = layers.Dense(4*p, activation='relu')(char_features) # feed forward network layer1
char_features1 = layers.Dense(p, activation='relu')(char_features1) # feed forward network layer2 
char_features = layers.add([char_features, char_features1])  # residual layer

char_features = tf.reduce_sum(char_features, 1) #converts word matrix to a vector of dimension f 
print(char_features.shape, " char features shape")

#WORDS stack of two multihead self-attention with position-wise feed forward network
multi = layers.MultiHeadAttention(num_heads=1,key_dim=2) 

word_features = multi(word_features, word_features)  
print(word_features.shape)

word_features1 = layers.Dense(4*d, activation='relu')(word_features) # feed forward network layer1
word_features1 = layers.Dense(d, activation='relu')(word_features1) # feed forward network layer2 
word_features = layers.add([word_features, word_features1])  # residual layer

multi = layers.MultiHeadAttention(num_heads=1,key_dim=2) 

word_features = multi(word_features, word_features) 
print(word_features.shape)

word_features1 = layers.Dense(4*d, activation='relu')(word_features) # feed forward network layer1
word_features1 = layers.Dense(d, activation='relu')(word_features1) # feed forward network layer2 
word_features = layers.add([word_features, word_features1])  # residual layer
word_features = tf.reduce_sum(word_features, 1) #converts word matrix to a vector of dimension d  #equation 12

x = layers.concatenate([word_features, char_features]) # concatenate word vector and char vector
print(x.shape," x shape")
dropout = layers.Dropout(drop)(x)
city_pred = layers.Dense(num_classes, name="city",activation="softmax")(x)   #change value of 2 to number of possible cities
# city_pred = tf.keras.layers.Softmax(city_pred)
country_pred = layers.Dense(num_classes_country, name="country",activation="softmax")(x)  #change value of 2 to number of possible countries
# country_pred = tf.keras.layers.Softmax(country_pred)
model = keras.Model(
    inputs=[word_input,char_input],
    outputs=[city_pred, country_pred],
)

model.compile(
    optimizer=keras.optimizers.Adam(0.001),
    loss={
        "city": keras.losses.CategoricalCrossentropy(from_logits=True),
        "country": keras.losses.CategoricalCrossentropy(from_logits=True),
    },
    loss_weights=[1.0, 1.0],
    metrics = ['accuracy']
)

history = model.fit(
    {"title": x_train,"body":x_char_train},
    {"city": y_train,"country":y_country_train},  # y_train in front of country should be changed 
    epochs=10,
    batch_size=512,
    verbose=1
)

# Evaluate the accuracy of our trained model
score = model.evaluate({"title": x_test,"body":x_char_test},
    {"city": y_test,"country": y_country_test}, batch_size=512, verbose=1)
print('Test loss:', score[0])
print('Test city accuracy:', score[3]*100)
print('Test country accuracy:', score[4]*100)

"""model3"""

#  num_words = 1000

drop = 0.3
word_input = keras.Input(
    shape=(None,), name="title"
)  # Variable-length sequence of ints
char_input = keras.Input(shape=(None,), name="body")  # Variable-length sequence of ints
d = 200    #dimension of a word
# Embed each word in the title into a 64-dimensional vector
word_features = layers.Embedding(1000, d)(word_input)

word_features = TrigPosEmbedding(
    output_dim=d,                      # The dimension of embeddings.
    mode=TrigPosEmbedding.MODE_ADD,  # Use `expand` mode
)(word_features)
# word_features = layers.add([word_features,pos_encoding])
print(word_features.shape)

# CHARS getting character encoding here instead for now only using words
char_features = layers.Embedding(1000, 100)(char_input)
print(char_features.shape, " char features")

#CHARS convolution for characters filters=100 kernel=3
res_conv = tf.keras.layers.Conv1D(64, 3, activation='relu',input_shape=(None,128))(char_features)
print(res_conv.shape, " res conv shape")

#CHARS max pooling layer after conv1D
res_pool = tf.keras.layers.MaxPooling1D(pool_size=3,strides=1, padding='valid')(res_conv)
print(res_pool.shape, " res pool shape")

#CHARS two multihead self-attention folloowed by feed forward neural network
multi = layers.MultiHeadAttention(num_heads=2,key_dim=2) 

char_features = multi(res_pool, res_pool)  # respool used two times for self-attention
print(word_features.shape)
p = 64
char_features1 = layers.Dense(4*p, activation='relu')(char_features) # feed forward network layer1
char_features1 = layers.Dense(p, activation='relu')(char_features1) # feed forward network layer2 
char_features = layers.add([char_features, char_features1])  # residual layer

print(char_features.shape, " char features shape")
multi = layers.MultiHeadAttention(num_heads=2,key_dim=2)
char_features = multi(char_features, char_features)  
char_features1 = layers.Dense(4*p, activation='relu')(char_features) # feed forward network layer1
char_features1 = layers.Dense(p, activation='relu')(char_features1) # feed forward network layer2 
char_features = layers.add([char_features, char_features1])  # residual layer

char_features = tf.reduce_sum(char_features, 1) #converts word matrix to a vector of dimension f 
print(char_features.shape, " char features shape")

#WORDS stack of two multihead self-attention with position-wise feed forward network
multi = layers.MultiHeadAttention(num_heads=2,key_dim=2) 

word_features = multi(word_features, word_features)  
print(word_features.shape)

word_features1 = layers.Dense(4*d, activation='relu')(word_features) # feed forward network layer1
word_features1 = layers.Dense(d, activation='relu')(word_features1) # feed forward network layer2 
word_features = layers.add([word_features, word_features1])  # residual layer

multi = layers.MultiHeadAttention(num_heads=2,key_dim=2) 

word_features = multi(word_features, word_features) 
print(word_features.shape)

word_features1 = layers.Dense(4*d, activation='relu')(word_features) # feed forward network layer1
word_features1 = layers.Dense(d, activation='relu')(word_features1) # feed forward network layer2 
word_features = layers.add([word_features, word_features1])  # residual layer
word_features = tf.reduce_sum(word_features, 1) #converts word matrix to a vector of dimension d  #equation 12

x = layers.concatenate([word_features, char_features]) # concatenate word vector and char vector
print(x.shape," x shape")
dropout = layers.Dropout(drop)(x)
city_pred = layers.Dense(num_classes, name="city",activation="softmax")(x)   #change value of 2 to number of possible cities
# city_pred = tf.keras.layers.Softmax(city_pred)
country_pred = layers.Dense(num_classes_country, name="country",activation="softmax")(x)  #change value of 2 to number of possible countries
# country_pred = tf.keras.layers.Softmax(country_pred)
model = keras.Model(
    inputs=[word_input,char_input],
    outputs=[city_pred, country_pred],
)

model.compile(
    optimizer=keras.optimizers.Adam(0.001),
    loss={
        "city": keras.losses.CategoricalCrossentropy(from_logits=True),
        "country": keras.losses.CategoricalCrossentropy(from_logits=True),
    },
    loss_weights=[1.0, 1.0],
    metrics = ['accuracy']
)

history = model.fit(
    {"title": x_train,"body":x_char_train},
    {"city": y_train,"country":y_country_train},  # y_train in front of country should be changed 
    epochs=10,
    batch_size=512,
    verbose=1
)

# Evaluate the accuracy of our trained model
score = model.evaluate({"title": x_test,"body":x_char_test},
    {"city": y_test,"country": y_country_test}, batch_size=512, verbose=1)
print('Test loss:', score[0])
print('Test city accuracy:', score[3]*100)
print('Test country accuracy:', score[4]*100)
